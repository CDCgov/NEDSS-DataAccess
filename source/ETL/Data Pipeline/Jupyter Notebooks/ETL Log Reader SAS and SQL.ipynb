{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6a9a1a0",
   "metadata": {},
   "source": [
    "## SAS Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2e6988",
   "metadata": {},
   "source": [
    "Reads ETL logs for SAS and SQL sprocs. \n",
    "The time calculation for both is the same, but the format and data acquisition is not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "605b1711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4cd37b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'username'\n",
    "# Load SAS log files into local system\n",
    "files = glob.glob('/Users/'+username+'/Desktop/Classic_ETL_log_7_8/*.log')\n",
    "# Eg.files = glob.glob('/Users/UpasanaPattnaik/Desktop/ETL/logs/log_modern/*.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c40ab14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/UpasanaPattnaik/Desktop/ETL/Run_4_50_PHC_SAS/sp_SLD_ALL_Hep.log',\n",
       " '/Users/UpasanaPattnaik/Desktop/ETL/Run_4_50_PHC_SAS/Drop_Create_Tables.log',\n",
       " '/Users/UpasanaPattnaik/Desktop/ETL/Run_4_50_PHC_SAS/MasterETL1.log']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compatible with MasterETL1.log and MasterETL2.log\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f866dffc",
   "metadata": {},
   "source": [
    "### Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36878a78",
   "metadata": {},
   "source": [
    "\tStart Time:     07JAN2024:11:08:41\n",
    "*****The Start codeset*********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdce8ca9",
   "metadata": {},
   "source": [
    "\tEnd Time:     07JAN2024:11:08:49\n",
    "*****The End codeset*********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4781eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract logic follows MasterETL1.log and MasterETL2.log pattern\n",
    "def extract_time(log_path):\n",
    "    #Regex\n",
    "    timestamp_pattern = re.compile(r'(\\d{2}[a-zA-Z]{3}\\d{4}:\\d{2}:\\d{2}:\\d{2})')\n",
    "    program_pattern = re.compile(r'The (Start|End) (\\S[^*]*)')\n",
    "\n",
    "    log = {'SAS':[], 'Start Time': [], 'End Time':[]}\n",
    "    sas_dict = {}\n",
    "    current_timestamp = None\n",
    "    \n",
    "    with open(log_path, 'r') as log_file:\n",
    "        for line in log_file:\n",
    "            sas_match = re.search(program_pattern, line)\n",
    "            timestamp_match = re.search(timestamp_pattern, line)\n",
    "\n",
    "            if timestamp_match:\n",
    "                timestamp_str = timestamp_match.group(1)\n",
    "                current_timestamp = datetime.strptime(timestamp_str, '%d%b%Y:%H:%M:%S')\n",
    "                \n",
    "            elif sas_match:\n",
    "                # To account for modern file format cases. \n",
    "                if current_timestamp is None:\n",
    "                    continue\n",
    "                action, sas_label = sas_match.groups()\n",
    "                current_sas = sas_label \n",
    "                #print(current_timestamp)\n",
    "                #print(\"current_sas: \", sas_label)\n",
    "                \n",
    "                if action == 'Start':\n",
    "                    sas_dict[current_sas] = current_timestamp\n",
    "                elif action == 'End' and current_sas in sas_dict:\n",
    "                    log['SAS'].append(current_sas)\n",
    "                    log['Start Time'].append(sas_dict[current_sas])\n",
    "                    log['End Time'].append(current_timestamp)\n",
    "            \n",
    "                    \n",
    "    df = pd.DataFrame(log)\n",
    "    df['Time Diff'] = (df['End Time'] - df['Start Time']).dt.total_seconds()\n",
    "    df['Hours'] = df['Time Diff']//3600\n",
    "    df['Minutes'] = (df['Time Diff']%3600)//60\n",
    "    df['Seconds'] = df['Time Diff']%60\n",
    "    \n",
    "    return df     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b0435e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_etl_log = extract_time(files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "91d1d7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAS</th>\n",
       "      <th>Start Time</th>\n",
       "      <th>End Time</th>\n",
       "      <th>Time Diff</th>\n",
       "      <th>Hours</th>\n",
       "      <th>Minutes</th>\n",
       "      <th>Seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PHCobservations</td>\n",
       "      <td>2024-01-17 15:49:54</td>\n",
       "      <td>2024-01-17 15:49:56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>InvestigationDim</td>\n",
       "      <td>2024-01-17 15:49:56</td>\n",
       "      <td>2024-01-17 15:50:06</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                SAS          Start Time            End Time  Time Diff  Hours   \n",
       "0   PHCobservations 2024-01-17 15:49:54 2024-01-17 15:49:56        2.0    0.0  \\\n",
       "1  InvestigationDim 2024-01-17 15:49:56 2024-01-17 15:50:06       10.0    0.0   \n",
       "\n",
       "   Minutes  Seconds  \n",
       "0      0.0      2.0  \n",
       "1      0.0     10.0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_etl_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51249439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_etl_log['Time Diff'].sum() #//3600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f49d8c",
   "metadata": {},
   "source": [
    "## SQL Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "280dad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from sqlalchemy.engine import URL\n",
    "import sqlalchemy as sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3668980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = #Provide host\n",
    "user = #Provide username\n",
    "password = #Provide password\n",
    "database = 'rdb_modern'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5753bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ODBC Driver 17 for SQL Server']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyodbc.drivers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "04004cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQLACLHEMY\n",
    "connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=\"+host+\";DATABASE=\"+database+\";UID=\"+user+\";PWD=\"+password\n",
    "connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(connection_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1f7cda9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.begin() as conn:\n",
    "    sql_df_og = pd.read_sql_query(sa.text(\"Select  * from rdb..job_flow_log jfl where batch_id = ******\"), conn)\n",
    "    #sql_df_og = pd.read_sql_query(sa.text(\"Select  * from rdb_modern..job_flow_log jfl where batch_id = ******\"), conn)\n",
    "sql_df = sql_df_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2dc6a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom name updates to standardize logs. Common throughout all ETL runs. \n",
    "\n",
    "#sp_D_LAB_TEST\n",
    "update_d_labtest = ['D_LabTest', 'RDB.D_LabTest']\n",
    "condition_d_labtest_1 = sql_df['Dataflow_Name'].isin(update_d_labtest)\n",
    "condition_d_labtest_2 = sql_df['package_Name'].isin(update_d_labtest)\n",
    "sql_df.loc[condition_d_labtest_1, 'Dataflow_Name'] = 'D_LABTEST'\n",
    "sql_df.loc[condition_d_labtest_2, 'package_Name'] = 'RDB.D_LABTEST'\n",
    "\n",
    "#sp_Vaccination_Record\n",
    "update_Vaccination_Record = ['Vaccination_Record']\n",
    "condition_Vaccination_Record = sql_df['Dataflow_Name'].isin(update_Vaccination_Record)\n",
    "sql_df.loc[condition_Vaccination_Record, 'Dataflow_Name'] = 'VaccinationRecord'\n",
    "\n",
    "#sp_D_Lab100\n",
    "update_Lab100 = ['Lab100','RDB.D_LAB100']\n",
    "condition_Lab100_1 = sql_df['Dataflow_Name'].isin(update_Lab100)\n",
    "condition_Lab100_2 = sql_df['package_Name'].isin(update_Lab100)\n",
    "sql_df.loc[condition_Lab100_1, 'Dataflow_Name'] = 'D_LAB100'\n",
    "sql_df.loc[condition_Lab100_2, 'Dataflow_Name'] = 'LAB100_DATAMART'\n",
    "sql_df.loc[condition_Lab100_2, 'package_Name'] = 'RDB.LAB100_DATAMART'\n",
    "\n",
    "#sp_Event_Metric\n",
    "update_EVENT_METRIC_DATAMART= ['EVENT_METRIC_INC DATAMART', 'RDB.dbo.EVENT_METRIC_INC']\n",
    "condition_EVENT_METRIC_DATAMART_1 = sql_df['Dataflow_Name'].isin(update_EVENT_METRIC_DATAMART)\n",
    "condition_EVENT_METRIC_DATAMART_2 = sql_df['package_Name'].isin(update_EVENT_METRIC_DATAMART)\n",
    "sql_df.loc[condition_EVENT_METRIC_DATAMART_1, 'Dataflow_Name'] = 'EVENT_METRIC DATAMART'\n",
    "sql_df.loc[condition_EVENT_METRIC_DATAMART_2, 'package_Name'] = 'RDB.dbo.EVENT_METRIC'\n",
    "\n",
    "#sp_D_Lab101\n",
    "update_LAB101_DATAMART = ['D_LAB101', 'RDB.D_LAB101']\n",
    "condition_LAB101_DATAMART_1 = sql_df['Dataflow_Name'].isin(update_LAB101_DATAMART)\n",
    "condition_LAB101_DATAMART_2 = sql_df['package_Name'].isin(update_LAB101_DATAMART)\n",
    "sql_df.loc[condition_LAB101_DATAMART_1, 'Dataflow_Name'] = 'LAB101_DATAMART'\n",
    "sql_df.loc[condition_LAB101_DATAMART_2, 'package_Name'] = 'RDB.LAB101_DATAMART'\n",
    "\n",
    "#sp_nbs_batch_start\n",
    "update_nbs_batch_start = ['nbs_batch_start']\n",
    "condition_nbs_batch_start = sql_df['package_Name'].isin(update_nbs_batch_start)\n",
    "sql_df.loc[condition_nbs_batch_start, 'package_Name'] = 'sp_nbs_batch_start'\n",
    "\n",
    "#sp_nbs_batch_complete\n",
    "update_nbs_batch_complete = ['sp_nbs_batch_complete']\n",
    "condition_nbs_batch_complete = sql_df['package_Name'].isin(update_nbs_batch_complete)\n",
    "sql_df.loc[condition_nbs_batch_complete, 'package_Name'] = 'nbs_BATCH_COMPLETE'\n",
    "\n",
    "#sp_F_STD_PAGE_CASE\n",
    "update_F_STD_PAGE_CASE = ['RDB.S_F_STD_PAGE_CASE', 'S_F_STD_PAGE_CASE', 'RDB.F_STD_PAGE_CASE']\n",
    "condition_F_STD_PAGE_CASE = sql_df['package_Name'].isin(update_F_STD_PAGE_CASE)\n",
    "sql_df.loc[condition_F_STD_PAGE_CASE, 'package_Name'] = 'F_STD_PAGE_CASE'\n",
    "\n",
    "#sp_F_PAGE_CASE\n",
    "update_F_PAGE_CASE = ['RDB.S_F_PAGE_CASE', 'S_F_PAGE_CASE', 'RDB.F_PAGE_CASE']\n",
    "condition_F_PAGE_CASE = sql_df['package_Name'].isin(update_F_PAGE_CASE)\n",
    "sql_df.loc[condition_F_PAGE_CASE, 'package_Name'] = 'F_PAGE_CASE'\n",
    "\n",
    "#sp_SLD_Investigation_Repeat\n",
    "update_INVESTIGATION_REPEAT_1 = ['RDB.D_INVESTIGATION_REPEAT']\n",
    "update_INVESTIGATION_REPEAT_2 = ['D_INVESTIGATION_REPEAT']\n",
    "condition_INVESTIGATION_REPEAT_1 = sql_df['package_Name'].isin(update_INVESTIGATION_REPEAT_1)\n",
    "condition_INVESTIGATION_REPEAT_2 = sql_df['package_Name'].isin(update_INVESTIGATION_REPEAT_2)\n",
    "sql_df.loc[condition_INVESTIGATION_REPEAT_1, 'package_Name'] = 'RDB.SLD_INVESTIGATION_REPEAT'\n",
    "sql_df.loc[condition_INVESTIGATION_REPEAT_2, 'package_Name'] = 'SLD_INVESTIGATION_REPEAT'\n",
    "\n",
    "#sp_STD_HIV_Datamart\n",
    "update_STD_HIV_DATAMART_1 = ['RDB.INV_HIV']\n",
    "update_STD_HIV_DATAMART_2 = ['INV_HIV']\n",
    "condition_STD_HIV_DATAMART_1 = sql_df['package_Name'].isin(update_STD_HIV_DATAMART_1)\n",
    "condition_STD_HIV_DATAMART_2 = sql_df['package_Name'].isin(update_STD_HIV_DATAMART_2)\n",
    "sql_df.loc[condition_STD_HIV_DATAMART_1, 'package_Name'] = 'RDB.STD_HIV_DATAMART'\n",
    "sql_df.loc[condition_STD_HIV_DATAMART_2, 'package_Name'] = 'STD_HIV_DATAMART'\n",
    "\n",
    "#sp_INV_SUMM_DATAMART\n",
    "update_INV_SUMM_DATAMART_1 = ['RDB.D_INV_Summ_DataMart']\n",
    "update_INV_SUMM_DATAMART_2 = ['D_INV_Summ_DataMart']\n",
    "condition_INV_SUMM_DATAMART_1 = sql_df['package_Name'].isin(update_INV_SUMM_DATAMART_1)\n",
    "condition_INV_SUMM_DATAMART_2 = sql_df['package_Name'].isin(update_INV_SUMM_DATAMART_2)\n",
    "sql_df.loc[condition_INV_SUMM_DATAMART_1, 'package_Name'] = 'RDB.INV_SUMM_DATAMART'\n",
    "sql_df.loc[condition_INV_SUMM_DATAMART_2, 'package_Name'] = 'INV_SUMM_DATAMART'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "22d36439",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_df = sql_df.sort_values(by=['Dataflow_Name', 'package_Name','Step_Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "563dedda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_group_mod(send_over):\n",
    "    step1_cdttm = send_over.loc[send_over['Step_Number'].isin([0,1]),'create_dttm'].min()\n",
    "    last_step = send_over.loc[send_over['Step_Number'] == send_over['Step_Number'].max(), 'update_dttm'].max()\n",
    "    return pd.Series(\n",
    "        {\n",
    "            'step1_cdttm': step1_cdttm,\n",
    "            'end_time': last_step,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3babab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_up = sql_df.groupby(['Dataflow_Name', 'package_Name']).apply(agg_group_mod).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d84033b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_up['time_diff_seconds'] = (result_df_up['end_time'] - result_df_up['step1_cdttm']).dt.total_seconds()\n",
    "result_df_up['Hours'] = result_df_up['time_diff_seconds']//3600\n",
    "result_df_up['Minutes'] = (result_df_up['time_diff_seconds']%3600)//60\n",
    "result_df_up['Seconds'] = result_df_up['time_diff_seconds']%60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483def80",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df_up.sort_values(by='step1_cdttm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df804c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Compare\n",
    "display(result_df_up.sort_values(by='step1_cdttm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "653cbe55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508.97599999999983"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_up['Seconds'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0557cee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_up.sort_values(by='step1_cdttm').to_csv('job_flow_log_7th.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
